# Datadog Dashboard Reference

Detailed reference for Datadog dashboard JSON structure, widget types, and patterns.

## Dashboard Root Structure

```json
{
  "title": "Service Name - generated by Cursor",
  "description": "[[suggested_dashboards]]",
  "widgets": [],
  "template_variables": [],
  "layout_type": "ordered",
  "notify_list": [],
  "pause_auto_refresh": false,
  "reflow_type": "fixed"
}
```

**Key Fields:**
- `title`: Dashboard title (service name) - **Always append " - generated by Cursor"** (e.g., `"Files Service - generated by Cursor"`)
- `description`: Usually `"[[suggested_dashboards]]"`
- `widgets`: Array of widget/group objects
- `layout_type`: `"ordered"` for main dashboard
- `reflow_type`: `"fixed"` to prevent auto-reflow

## Widget Types

### Timeseries Widget

For displaying metric data over time.

```json
{
  "id": <unique_id>,
  "definition": {
    "title": "Metric Name",
    "title_size": "16",
    "title_align": "left",
    "show_legend": true,
    "legend_layout": "auto",
    "legend_columns": ["avg", "min", "max", "value", "sum"],
    "type": "timeseries",
    "requests": [{
      "formulas": [{"formula": "query1"}],
      "queries": [{
        "name": "query1",
        "data_source": "metrics",
        "query": "avg:metric.name{service:service-name} by {tag}"
      }],
      "response_format": "timeseries",
      "style": {
        "palette": "dog_classic",
        "order_by": "values",
        "line_type": "solid",
        "line_width": "normal"
      },
      "display_type": "line"
    }],
    "yaxis": {
      "include_zero": true,
      "max": "100"
    }
  },
  "layout": {"x": 0, "y": 0, "width": 4, "height": 2}
}
```

**Display Types:**
- `"line"`: Line chart (default)
- `"bars"`: Bar chart
- `"area"`: Area chart

**Palettes:**
- `"dog_classic"`: Default color palette
- `"semantic"`: Green/yellow/red for status metrics
- `"cool"`, `"warm"`, `"purple"`, `"orange"`, `"green"`: Color themes

### List Stream Widget

For displaying log events.

```json
{
  "id": <unique_id>,
  "definition": {
    "title": "Errors",
    "title_size": "16",
    "title_align": "left",
    "requests": [{
      "response_format": "event_list",
      "query": {
        "data_source": "logs_stream",
        "query_string": "service:service-name status:error",
        "indexes": [],
        "storage": "hot"
      },
      "columns": [
        {"field": "status_line", "width": "auto"},
        {"field": "timestamp", "width": "auto"},
        {"field": "host", "width": "auto"},
        {"field": "service", "width": "auto"},
        {"field": "content", "width": "auto"}
      ]
    }],
    "type": "list_stream"
  },
  "layout": {"x": 0, "y": 0, "width": 8, "height": 4}
}
```

### Group Widget

For organizing related widgets.

```json
{
  "id": <unique_id>,
  "definition": {
    "title": "Group Title",
    "show_title": true,
    "type": "group",
    "layout_type": "ordered",
    "widgets": [
      // Array of widget definitions
    ]
  },
  "layout": {"x": 0, "y": 0, "width": 12, "height": <calculated_height>}
}
```

**Note:** Groups contain widgets directly in their `definition.widgets` array, not in separate layout objects.

### Powerpack Widget

For pre-built dashboard components.

```json
{
  "id": <unique_id>,
  "definition": {
    "title": "Resource usage",
    "background_color": "purple",
    "show_title": true,
    "powerpack_id": "50dbb12a-c818-11ed-a984-da7ad0900005",
    "template_variables": {
      "controlled_externally": [],
      "controlled_by_powerpack": [
        {"name": "kube_namespace", "prefix": "kube_namespace", "values": ["production"]},
        {"name": "kube_deployment", "prefix": "kube_deployment", "values": ["service-name"]}
      ]
    },
    "type": "powerpack"
  },
  "layout": {"x": 0, "y": 0, "width": 12, "height": 9, "is_column_break": true}
}
```

## Powerpack IDs

### Resource Usage Powerpack
- **ID**: `50dbb12a-c818-11ed-a984-da7ad0900005`
- **Purpose**: CPU, memory, disk usage monitoring
- **Template Variables**: `kube_namespace`, `kube_deployment`
- **Background Color**: `"purple"`

### Kafka Lag Powerpack
- **ID**: `02a5c08a-34b4-11f0-8eb6-da7ad0900005`
- **Purpose**: Kafka consumer lag monitoring
- **Template Variables**: `cluster` (prefix "cluster_name"), `topic` (prefix "topic")
- **Topic values**: The `topic` variable accepts **multiple values** (array of topic names) to restrict the powerpack to those topics. When generating a **service-specific** dashboard, set `topic.values` to the list of topics discovered from that service's repository (see SKILL.md Step 1.2).
- **Background Color**: `"vivid_orange"`

### Kafka Throughput Powerpack
- **ID**: `b56f52e8-34b0-11f0-a297-da7ad0900005`
- **Purpose**: Kafka message throughput monitoring
- **Template Variables**: `cluster` (prefix "cluster_name"), `topic` (prefix "topic")
- **Topic values**: The `topic` variable accepts **multiple values** (array of topic names) to restrict the powerpack to those topics. When generating a **service-specific** dashboard, set `topic.values` to the list of topics discovered from that service's repository (see SKILL.md Step 1.2).
- **Background Color**: `"vivid_orange"`

## Common Metric Query Patterns

### HikariCP Connection Pool Metrics

```datadog
# Active connections
avg:hikaricp.connections.active{service:service-name} by {host}

# Idle connections
avg:hikaricp.connections.idle{service:service-name} by {host}

# Total connections
avg:hikaricp.connections{service:service-name} by {host}

# Pending connections (waiting)
avg:hikaricp.connections.pending{service:service-name} by {host}

# Connection acquire P95
avg:hikaricp.connections.acquire.95percentile{service:service-name} by {host}

# Connection creation P95
avg:hikaricp.connections.creation.95percentile{service:service-name} by {host}

# Connection usage P95
avg:hikaricp.connections.usage.95percentile{service:service-name} by {host}

# Connection timeouts
sum:hikaricp.connections.timeout{service:service-name} by {host}.as_count()

# Pool utilization percentage
(sum:hikaricp.connections.active{service:service-name} by {service}.weighted() / 
 sum:hikaricp.connections.max{service:service-name} by {service}.weighted()) * 100
```

### Kafka/Eventstream Metrics

```datadog
# Average processing time
avg:kafka.subscriber.bulk.processing.time.avg{topic:topic-name} by {group.id}

# Processing time P95
avg:kafka.subscriber.bulk.processing.time.95percentile{topic:topic-name} by {group.id}

# Processing time P99
avg:kafka.subscriber.bulk.processing.time.99percentile{topic:topic-name} by {group.id}

# Processing time max
avg:kafka.subscriber.bulk.processing.time.max{topic:topic-name} by {group.id}

# Bulk size average
avg:kafka.subscriber.bulk.size.avg{topic:topic-name} by {group.id}

# Bulk processing rate (bulks/sec)
sum:kafka.subscriber.bulk.processing.time.count{topic:topic-name} by {group.id}.as_count()
```

### AWS SQS Metrics

```datadog
# Messages visible
avg:aws.sqs.approximate_number_of_messages_visible{queuename:queue-name}

# Age of oldest message
avg:aws.sqs.approximate_age_of_oldest_message{queuename:queue-name}

# Messages received
sum:aws.sqs.number_of_messages_received{queuename:queue-name}.as_count()
```

### AWS SNS Metrics

```datadog
# Messages published
sum:aws.sns.number_of_messages_published{topicname:topic-name}.as_count()
```

### Database Metrics

```datadog
# PostgreSQL database size
avg:postgresql.database_size{db:database-name}

# PostgreSQL query time
sum:postgresql.queries.time{db:database-name,user:user-name} by {query}.as_count().rollup(10)

# PostgreSQL query count
sum:postgresql.queries.count{db:database-name,user:user-name} by {query}.as_count().rollup(10)
```

### Cache Metrics

Cache metrics follow Micrometer cache metrics naming convention. Common patterns:

**Cache Hit Rate (with rollup for accuracy):**
```datadog
# Cache hit rate percentage (recommended pattern)
(sum:cache.gets{cache:cache-name,result:hit,service:service-name} by {service}.as_count().rollup(avg) / 
 (sum:cache.gets{cache:cache-name,result:hit,service:service-name} by {service}.as_count().rollup(avg) + 
  sum:cache.gets{cache:cache-name,result:miss,service:service-name} by {service}.as_count().rollup(avg))) * 100

# Alternative: Simple hit rate (without rollup)
(sum:cache.gets{cache:cache-name,result:hit} by {service}.as_count() / 
 (sum:cache.gets{cache:cache-name,result:hit} by {service}.as_count() + 
  sum:cache.gets{cache:cache-name,result:miss} by {service}.as_count())) * 100
```

**Cache Operations:**
```datadog
# Cache hits
sum:cache.gets{cache:cache-name,result:hit,service:service-name} by {service}.as_count().rollup(avg)

# Cache misses
sum:cache.gets{cache:cache-name,result:miss,service:service-name} by {service}.as_count().rollup(avg)

# Total cache gets
sum:cache.gets{cache:cache-name,service:service-name} by {service}.as_count()
```

**Cache Size and Evictions:**
```datadog
# Cache size (gauge metric)
avg:cache.size{cache:cache-name,service:service-name} by {service}

# Cache evictions (counter metric)
avg:cache.evictions{cache:cache-name,service:service-name} by {service}.as_count()
```

**Cache Hit Rate with Cutoff (for visualization):**
```datadog
# Cache hit rate capped at 80% (shows values below threshold)
cutoff_max((query1 / (query1 + query2) * 100), 80)
```

**Cache Detection Patterns:**
- Search codebase for: `@Cacheable`, `Cache`, `cache.gets`, `cache.size`, `cache.evictions`
- Check build files for cache libraries: Caffeine, Guava Cache, Spring Cache
- Look for cache metric registrations in code
- Identify cache names from configuration or code annotations

**Common Cache Types:**
- Client-side caches: Often prefixed with service name (e.g., `compound_company_toggles`)
- Server-side caches: May use descriptive names (e.g., `featuretoggles`, `companydetailsforld`)
- Legacy vs compound: Services may have both legacy and compound cache implementations

## Layout System

### Grid Layout

- **Width**: 12 columns (0-11)
- **Height**: Variable units
- **X coordinate**: 0-11 (left to right)
- **Y coordinate**: 0+ (top to bottom, cumulative)

### Layout Object

```json
{
  "x": 0,      // Column position (0-11)
  "y": 0,      // Row position (cumulative)
  "width": 4,  // Column span (1-12)
  "height": 2, // Row span (1+)
  "is_column_break": false  // Optional: force new column
}
```

### Layout Best Practices

- Groups typically span full width: `"width": 12`
- Widgets within groups: `"width": 3` or `"width": 4` for 3-4 widgets per row
- Standard widget height: `"height": 2` or `"height": 3`
- Use `"is_column_break": true` for powerpacks to start new column

## Formula Functions

Common Datadog formula functions:

- `query1`: Reference first query
- `query2`: Reference second query
- `.as_count()`: Convert to count metric
- `.rollup(avg, 60)`: Rollup aggregation
- `calendar_shift(query0, '-1w', 'Europe/Bucharest')`: Compare with previous week
- `cutoff_max(value, threshold)`: Cap maximum value
- `ewma_10(value)`: Exponential weighted moving average
- `autosmooth(query1)`: Auto-smoothing

## Widget ID Generation

Generate unique IDs for widgets:
- Use timestamp-based: `Date.now()` or similar
- Use sequential: Start from large number (e.g., 1000000000000001)
- Ensure uniqueness across all widgets in dashboard

## Environment Tags

**CRITICAL: NEVER use `env:prod` or any environment tags in metric queries**

- **ALWAYS use `{service:<service-name>}` tag in all metric queries**
- This is a hard rule with no exceptions

## Service Name Patterns

- Extract from `service.datadog.yaml`: `dd-service` field
- Fallback: Repository name (kebab-case)
- Use in queries: `service:{service-name}`
- Verify service name matches actual Datadog service tag

## Common Widget Configurations

### Status Metrics (Semantic Palette)

```json
{
  "style": {
    "palette": "semantic",
    "order_by": "values",
    "line_type": "solid",
    "line_width": "normal"
  },
  "display_type": "bars"
}
```

### Performance Metrics (Dog Classic)

```json
{
  "style": {
    "palette": "dog_classic",
    "order_by": "values",
    "line_type": "solid",
    "line_width": "normal"
  },
  "display_type": "line"
}
```

### Percentage Metrics (with Y-axis max)

```json
{
  "yaxis": {
    "include_zero": true,
    "max": "100"
  }
}
```

## Log Marker Charts

Charts created from logs with log markers (not metrics). This allows monitoring important operations that are logged but don't have dedicated metrics.

### Log Marker Pattern

**Scala:**
```scala
logger.info("report metadata retrieval", 
  "companyId" -> companyId, 
  "viewType" -> viewType, 
  "responseSizeBytes" -> responseSizeBytes)
```

**Kotlin:**
```kotlin
logger.info("S3 PUT operation to bucket", 
  "operation" to "PUT", 
  "bucket" to bucketName, 
  "status" to status, 
  "count" to count)
```

**Datadog Format:**
- Markers become `@bobData.*` fields in Datadog logs
- Example: `"bucket" to bucketName` becomes `@bobData.bucket`
- Example: `"viewType" -> viewType` becomes `@bobData.viewType`

### Datadog Log Query Structure

**Widget Type:** `timeseries` with `data_source: "logs"`

**Query Structure:**
```json
{
  "name": "query1",
  "data_source": "logs",
  "search": {
    "query": "<log-message> @logger.name:*.<ClassName>"
  },
  "indexes": ["*"],
  "group_by": {
    "fields": ["@bobData.<marker-field>"],
    "limit": 1000,
    "sort": {
      "aggregation": "count",
      "metric": "count",
      "order": "desc"
    }
  },
  "compute": {
    "aggregation": "<avg|max|count>",
    "metric": "@bobData.<numeric-field>"
  },
  "storage": "hot"
}
```

**Key Components:**
- `search.query`: Log message text + optional `@logger.name:*.ClassName` filter (use wildcard pattern because logs may have abbreviated package names)
- `group_by.fields`: Array of `@bobData.fieldName` fields for grouping
- `compute`: Aggregation (`avg`, `max`, `count`) on numeric marker fields
- `indexes`: `["*"]` to search all log indexes

### Common Aggregations

- **Count**: `{"aggregation": "count"}` - Count log entries
- **Average**: `{"aggregation": "avg", "metric": "@bobData.responseSizeBytes"}` - Average of numeric marker
- **Max**: `{"aggregation": "max", "metric": "@bobData.responseSizeBytes"}` - Maximum of numeric marker

### Detection Patterns

**Scala:**
- Pattern: `logger.(info|debug|warn|error).*\"[^\"]+\".*->`
- Extract log message and markers from `->` syntax

**Kotlin:**
- Pattern: `logger.(info|debug|warn|error).*\"[^\"]+\".*to`
- Extract log message and markers from `to` syntax

**Filtering Rules:**
- Skip if only `companyId` and/or `employeeId` markers (must have at least one other marker)
- Prefer logs with numeric markers for aggregation (size, count, duration, bytes, etc.)
- Prioritize logs with `metricName` marker (indicates intentional logging for metrics)
- Prioritize logs with 2+ markers (more context for grouping/aggregation)
- Skip test files (`*Test.kt`, `*Test.scala`, `*Spec.kt`, `*Spec.scala`)
- Skip debug/trace level logs (unless they have `metricName` marker)
- Limit to top 5-10 most valuable logs per service

**Prioritization:**
1. Logs with `metricName` marker (highest priority)
2. Logs in `*Metrics.kt` or `*Telemetry.scala` classes
3. Logs with numeric markers + 2+ total markers
4. Logs with 2+ non-ID markers

### Example Widget

**From ReportsDashboard:**
```json
{
  "definition": {
    "title": "Report Metadata - Response Size by View Type",
    "type": "timeseries",
    "requests": [{
      "queries": [{
        "name": "query1",
        "data_source": "logs",
        "search": {
          "query": "report metadata retrieval @logger.name:*.ReportMetaDataController"
        },
        "group_by": {
          "fields": ["@bobData.viewType"]
        },
        "compute": {
          "aggregation": "avg",
          "metric": "@bobData.responseSizeBytes"
        }
      }],
      "formulas": [{"formula": "query1"}]
    }]
  }
}
```

### Best Practices

- Use `@logger.name:*.ClassName` filter to narrow down logs (wildcard pattern because logs may have abbreviated package names like `c.h.f.common.FileMetrics` instead of `com.hibob.files.common.FileMetrics`)
- Prefer numeric markers for aggregation (avg, max)
- Use categorical markers for grouping (bucket, operation, status)
- Avoid charts with only companyId/employeeId grouping (too granular)
- Limit to 1-3 widgets per log to avoid dashboard overload
- Quality over quantity - only include logs that provide meaningful insights

## Sample Dashboard References

See sample dashboards in `samples/` directory:
- `audit-service.json`: Complex service with Kafka, SQS, business metrics
- `hikaricp-metrics.json`: Connection pool monitoring patterns
- `tx-outbox.json`: Transactional outbox patterns with Postgres/SingleStore
- `toggles-dashboard.json`: Cache metrics, template variables, APM metrics
- `email-service.json`: Service-specific patterns
- `ReportsDashboard`: Log marker charts using `data_source: "logs"` with `@bobData.*` fields
